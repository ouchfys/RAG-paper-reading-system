import streamlit as st
import torch
from transformers import AutoModel, AutoTokenizer
from langchain_community.embeddings import HuggingFaceEmbeddings
try:
    from langchain_community.vectorstores import Neo4jVector
except ImportError:
    from langchain.vectorstores import Neo4jVector
from py2neo import Graph
import sys
import os

# æ·»åŠ systemç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'system'))

from rag_pipeline import AdvancedRAGPipeline
from prompt_templates import PromptTemplateManager, SelfConsistencyVerifier

# ================= é…ç½®è·¯å¾„ =================
MODEL_PATH = "/root/autodl-tmp/models/chatglm3-6b"
EMBEDDING_PATH = "/root/autodl-tmp/models/bge-large-zh-v1.5"
NEO4J_URL = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "12345678"

# è®¾ç½®é¡µé¢
st.set_page_config(page_title="é«˜çº§RAGç³»ç»Ÿ", page_icon="ğŸš€", layout="wide")
st.title("ğŸš€ é«˜çº§å·¥ä¸šæ–‡æ¡£çŸ¥è¯†é—®ç­”ç³»ç»Ÿ (Advanced RAG)")
st.markdown("### ğŸ”¥ å›¾æ•°æ®åº“æ·±åº¦åˆ©ç”¨ | HyDE | Query Rewriting | Re-ranking | Prompt Engineering")

# ================= 1. åŠ è½½æ¨¡å‹ =================
@st.cache_resource
def load_models():
    print("â³ [System] æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
    try:
        embedding = HuggingFaceEmbeddings(
            model_name=EMBEDDING_PATH,
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
        )
    except Exception as e:
        st.error(f"Embedding æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None, None
    
    print("â³ [System] æ­£åœ¨è¿æ¥ Neo4j...")
    try:
        vector_store = Neo4jVector.from_existing_graph(
            embedding=embedding,
            url=NEO4J_URL,
            username=NEO4J_USER,
            password=NEO4J_PASSWORD,
            index_name="vector",
            node_label="Chunk",
            text_node_properties=["text"],
            embedding_node_property="embedding",
        )
        
        # py2neo Graphè¿æ¥
        graph = Graph(NEO4J_URL, auth=(NEO4J_USER, NEO4J_PASSWORD))
    except Exception as e:
        st.error(f"Neo4j è¿æ¥å¤±è´¥: {e}")
        return None, None, None, None, None

    print("â³ [System] æ­£åœ¨åŠ è½½ ChatGLM3 æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModel.from_pretrained(
            MODEL_PATH, 
            trust_remote_code=True, 
            device_map="auto", 
            torch_dtype="auto"
        ).eval()
    except Exception as e:
        st.error(f"ChatGLM3 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None, None
    
    print("â³ [System] åˆå§‹åŒ–é«˜çº§RAGæµæ°´çº¿...")
    try:
        pipeline = AdvancedRAGPipeline(model, tokenizer, vector_store, graph)
        consistency_verifier = SelfConsistencyVerifier(model, tokenizer)
    except Exception as e:
        st.error(f"RAGæµæ°´çº¿åˆå§‹åŒ–å¤±è´¥: {e}")
        return None, None, None, None, None
    
    print("âœ… [System] æ‰€æœ‰ç»„ä»¶åŠ è½½å®Œæˆ!")
    return tokenizer, model, vector_store, pipeline, consistency_verifier

tokenizer, model, vector_store, pipeline, consistency_verifier = load_models()

# ================= 2. çŠ¶æ€ç®¡ç† =================
if "history" not in st.session_state:
    st.session_state.history = []

# ================= 3. ä¾§è¾¹æ é…ç½® =================
with st.sidebar:
    st.header("âš™ï¸ é«˜çº§é…ç½®")
    
    # æ£€ç´¢ç­–ç•¥é€‰æ‹©
    st.subheader("ğŸ” æ£€ç´¢ç­–ç•¥")
    retrieval_strategy = st.selectbox(
        "é€‰æ‹©æ£€ç´¢ç­–ç•¥",
        ["simple", "graph", "hybrid", "hyde", "multi_query", "full"],
        index=5,  # é»˜è®¤é€‰æ‹©full
        help="""
        - simple: ç®€å•å‘é‡æ£€ç´¢ (åŸºçº¿)
        - graph: å›¾å¢å¼ºæ£€ç´¢ (ä¸Šä¸‹æ–‡æ‰©å±•)
        - hybrid: æ··åˆæ£€ç´¢ (å‘é‡+å…³é”®è¯+å›¾)
        - hyde: HyDEæ£€ç´¢ (å‡æƒ³ç­”æ¡ˆ)
        - multi_query: å¤šæŸ¥è¯¢èåˆ
        - full: å®Œæ•´æµæ°´çº¿ (æ¨è)
        """
    )
    
    # Promptç­–ç•¥é€‰æ‹©
    st.subheader("ğŸ’¬ Promptç­–ç•¥")
    prompt_strategy = st.selectbox(
        "é€‰æ‹©Promptç±»å‹",
        ["default", "cot", "react", "few_shot"],
        index=0,
        help="""
        - default: æ ‡å‡†RAG prompt
        - cot: Chain-of-Thought (é€æ­¥æ¨ç†)
        - react: ReActæ¡†æ¶ (æ¨ç†+è¡ŒåŠ¨)
        - few_shot: Few-shot Learning (ç¤ºä¾‹å¼•å¯¼)
        """
    )
    
    # è‡ªæ´½æ€§éªŒè¯
    use_consistency = st.checkbox(
        "å¯ç”¨è‡ªæ´½æ€§éªŒè¯",
        value=False,
        help="ç”Ÿæˆå¤šä¸ªç­”æ¡ˆå¹¶é€‰æ‹©æœ€ä¸€è‡´çš„ (è€—æ—¶è¾ƒé•¿)"
    )
    
    if use_consistency:
        num_samples = st.slider("ç”Ÿæˆç­”æ¡ˆæ•°é‡", 2, 5, 3)
    
    # æ£€ç´¢å‚æ•°
    st.subheader("ğŸ›ï¸ æ£€ç´¢å‚æ•°")
    top_k = st.slider("è¿”å›æ–‡æ¡£æ•°é‡ (top_k)", 1, 10, 5)
    
    # æ¸…ç©ºæŒ‰é’®
    st.divider()
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.history = []
        st.rerun()
    
    st.info("ğŸ’¡ ä½¿ç”¨é«˜çº§æ£€ç´¢ç­–ç•¥å’ŒPromptå·¥ç¨‹æŠ€æœ¯ï¼Œæå‡é—®ç­”è´¨é‡")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for query, response, metadata in st.session_state.history:
    with st.chat_message("user"):
        st.markdown(query)
    with st.chat_message("assistant"):
        st.markdown(response)
        if metadata:
            with st.expander("ğŸ“Š æ£€ç´¢è¯¦æƒ…"):
                st.write(f"**æ£€ç´¢ç­–ç•¥**: {metadata.get('strategy', 'N/A')}")
                st.write(f"**Promptç±»å‹**: {metadata.get('prompt_type', 'N/A')}")
                st.write(f"**æ£€ç´¢æ–‡æ¡£æ•°**: {metadata.get('num_docs', 'N/A')}")

# ================= 4. æ ¸å¿ƒé—®ç­”é€»è¾‘ =================
if prompt_text := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    with st.chat_message("user"):
        st.markdown(prompt_text)

    # æ£€ç´¢
    context_docs = []
    print(f"ğŸ” ç”¨æˆ·æé—®: {prompt_text}")
    
    try:
        with st.status("ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...", expanded=True) as status:
            if pipeline:
                # ä½¿ç”¨é«˜çº§RAGæµæ°´çº¿æ£€ç´¢
                status.write(f"ğŸ“Œ ä½¿ç”¨æ£€ç´¢ç­–ç•¥: {retrieval_strategy}")
                context_docs = pipeline.retrieve(
                    prompt_text,
                    strategy=retrieval_strategy,
                    top_k=top_k
                )
                
                if context_docs:
                    status.write(f"âœ… æ£€ç´¢åˆ° {len(context_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                    for i, doc in enumerate(context_docs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        source = doc.metadata.get('source', 'æœªçŸ¥')
                        chapter = doc.metadata.get('chapter', 'æœªçŸ¥ç« èŠ‚')
                        page = doc.metadata.get('page', '?')
                        status.markdown(f"**ç‰‡æ®µ {i+1}**: {source} - {chapter} (P{page})")
                        status.code(doc.page_content[:150] + "...", language="text")
                else:
                    status.write("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")
                
                status.update(label="æ£€ç´¢å®Œæˆ", state="complete", expanded=False)
            else:
                status.write("âš ï¸ RAGæµæ°´çº¿æœªåˆå§‹åŒ–")
                status.update(label="æ£€ç´¢è·³è¿‡", state="error", expanded=False)
                
    except Exception as e:
        st.error(f"æ£€ç´¢å‡ºé”™: {e}")
        print(f"âŒ æ£€ç´¢å‡ºé”™: {e}")

    # æ„é€  Prompt
    if context_docs:
        input_prompt = PromptTemplateManager.build_rag_prompt(
            prompt_text,
            context_docs,
            prompt_type=prompt_strategy
        )
    else:
        input_prompt = f"é—®é¢˜: {prompt_text}\n\nè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”:"

    # ç”Ÿæˆç­”æ¡ˆ
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        
        if model and tokenizer:
            try:
                if use_consistency:
                    # ä½¿ç”¨è‡ªæ´½æ€§éªŒè¯
                    with st.spinner(f"ğŸ§  æ­£åœ¨ç”Ÿæˆ {num_samples} ä¸ªç­”æ¡ˆå¹¶éªŒè¯ä¸€è‡´æ€§..."):
                        full_response = consistency_verifier.generate_with_consistency(
                            input_prompt,
                            num_samples=num_samples,
                            temperature=0.7
                        )
                        placeholder.markdown(full_response)
                else:
                    # æ ‡å‡†ç”Ÿæˆ
                    for response, history, past_key_values in model.stream_chat(
                        tokenizer, 
                        input_prompt, 
                        history=[], 
                        do_sample=False,
                        repetition_penalty=1.2,
                        max_length=4096,
                        past_key_values=None,
                        return_past_key_values=True
                    ):
                        placeholder.markdown(response)
                        full_response = response
                
                # ä¿å­˜åˆ°å†å²
                metadata = {
                    'strategy': retrieval_strategy,
                    'prompt_type': prompt_strategy,
                    'num_docs': len(context_docs)
                }
                st.session_state.history.append((prompt_text, full_response, metadata))
                
                # æ˜¾ç¤ºå…ƒä¿¡æ¯
                with st.expander("ğŸ“Š æœ¬æ¬¡æ£€ç´¢è¯¦æƒ…"):
                    st.write(f"**æ£€ç´¢ç­–ç•¥**: {retrieval_strategy}")
                    st.write(f"**Promptç±»å‹**: {prompt_strategy}")
                    st.write(f"**æ£€ç´¢æ–‡æ¡£æ•°**: {len(context_docs)}")
                    if use_consistency:
                        st.write(f"**è‡ªæ´½æ€§éªŒè¯**: æ˜¯ ({num_samples}ä¸ªæ ·æœ¬)")
                
            except Exception as e:
                st.error(f"ç”Ÿæˆå‡ºé”™: {e}")
                print(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
        else:
            st.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚")
